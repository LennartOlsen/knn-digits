{
    "collab_server" : "",
    "contents" : "#-------------------------------------------------------------\n#load libraries\n#-------------------------------------------------------------\nlibrary(\"png\")\nlibrary(\"EBImage\")\nlibrary(\"class\")\nlibrary(\"gmodels\")\nlibrary(\"ggplot2\")\nlibrary(\"caret\")\n#library(\"lda\")\n#library(\"neuralnet\")\n#library(\"RSNNS\")\n#install.packages(\"neuralnet\")\n\n\n#This file contains 2 functions and some example code in the bottom\n#for using the 2 functions.\n#The example code use the functions to load and smoothen the image data.\n#lastly it converts the data into a table more suitable for R based classification \n\n\n#-------------------------------------------------------------\n#Smoothing function (you are welcome to use alternative functions from R)\n#-------------------------------------------------------------\nsmoothImage <- function(grayImg){\n  #two ways of specifying kernel:\n  # kernel <- matrix( \n  #           c(1, 1, 1, \n  #             1, 1, 1, \n  #             1, 1, 1), # the data elements \n  #           3,              # number of rows \n  #           3)\n  # kernel <- kernel/9\n  # kernel\n  kernel <- matrix( \n    1, # the data elements \n    3,# number of rows \n    3)\n  kernel <- kernel/9\n  #print(kernel)\n  \n  #using r library for smoothing\n  smoothed <- filter2(grayImg, kernel)\n  \n  #simple implementation of average filter:\n  # imgWidth <- length(gray[1,])\n  # imgHeight <- length(gray[,1])\n  # kernelSize <- 1\n  # for(px in 1:imgWidth)\n  # {\n  #   for(py in 1:imgHeight)\n  #   {\n  #     baseX <- px - kernelSize\n  #     endX <- px + kernelSize\n  #     if(baseX < 1){baseX<-1}\n  #     if(endX > imgWidth){endX<-imgWidth}\n  #     \n  #     baseY <- py - kernelSize\n  #     endY <- py + kernelSize\n  #     if(baseY < 1){baseY<-1}\n  #     if(endY > imgHeight){endY<-imgHeight}\n  #     \n  #     \n  #     value <- 0\n  #     for(pkx in baseX:endX)\n  #     {\n  #       for(pky in baseY:endY)\n  #       {\n  #         value <- value+gray[pky,pkx]\n  #       }\n  #     }\n  #     kernelValues <- (endY-baseY+1)*(endX-baseX+1)    \n  #     value <- value/kernelValues\n  #     \n  #     smoothed[py,px] <- value\n  #   }\n  # }\n  return(smoothed)\n}\n\n\n#-------------------------------------------------------------\n#Data loading function.\n#This function load a single dataset where the data is\n#specified by the DPI of the scanned images, the group number of the person\n#and the number of the group member.\n#Then the function use the smoothing function above, in order to smoothen the data.\n#The function then places a grid around the digit cells, based on the corner file,\n#so that it can be verified the data is read in a reasonable maner.\n#Lastly a vector of table is returned. Each table in this vector represents\n#a single type of digit, so the first table is only 0's, the second is 1's and so on.\n#In the tables the rows represent the individual handwritten digits.\n#The columns represents the pixel values.\n#-------------------------------------------------------------\nloadSinglePersonsData <- function(DPI,groupNr,groupMemberNr,folder){\n  #load the scaned images\n\n  ciffers <- list(readPNG(paste(c(folder,groupNr,\"/member\",groupMemberNr,\"/Ciphers\",DPI,\"-0.png\"), collapse = \"\")),\n                  readPNG(paste(c(folder,groupNr,\"/member\",groupMemberNr,\"/Ciphers\",DPI,\"-1.png\"), collapse = \"\")),\n                  readPNG(paste(c(folder,groupNr,\"/member\",groupMemberNr,\"/Ciphers\",DPI,\"-2.png\"), collapse = \"\")),\n                  readPNG(paste(c(folder,groupNr,\"/member\",groupMemberNr,\"/Ciphers\",DPI,\"-3.png\"), collapse = \"\")),\n                  readPNG(paste(c(folder,groupNr,\"/member\",groupMemberNr,\"/Ciphers\",DPI,\"-4.png\"), collapse = \"\")))\n  #load the corner values\n  corners <- read.csv(paste(c(folder,groupNr,\"/member\",groupMemberNr,\"/Corners.txt\"), collapse = \"\"))\n  corners <- trunc(corners*DPI/300)\n  #print(corners)\n  \n  #define lists to be used\n  #  gray <- list(1:5)\n  #   smoothed <- list(1:5)\n  prepared <- list(1:5)\n  \n  \n  #convert the images to gray scale.\n  for(i in 1:5)\n  {\n    r <-ciffers[[i]][,,1]\n    g <-ciffers[[i]][,,2]\n    b <-ciffers[[i]][,,3]\n    prepared[[i]] <- (r+g+b)/3\n  }\n  \n  #smooth images based on the funtion in the top\n  for(i in 1:5)\n  {\n    prepared[[i]] <- smoothImage(prepared[[i]])\n  }  \n  \n  #extract individual ciffers\n  #xStep and yStep is used to ensure the first corner of the\n  #individual ciffers are placed fairly accurate\n  xStep  <- (corners[1,7]-corners[1,1])/20;\n  yStep  <- (corners[1,8]-corners[1,2])/20;\n  \n  #xStepT and yStepT is used to ensure that the feature vectors\n  #from all people have the same size.\n  xStepT <- 60*DPI/300\n  yStepT <- 60*DPI/300\n  \n  dataMatrix <- matrix(1:((xStepT-2)*(yStepT-2) + 1)*10*20*20, nrow=10*20*20, ncol=(xStepT-2)*(yStepT-2) + 1)\n\n  for(pages in 1:5)\n  {\n    for(box in 1:2)\n    {\n      for(cifX in 1:20)\n      {\n        aXbase <- corners[(pages-1)*2 + box,1] + xStep*(cifX-1)\n        for(cifY in 1:20)\n        {\n          aYbase <- corners[(pages-1)*2 + box,2] + yStep*(cifY-1)\n         \n          dataMatrix[((pages-1)*2 + box - 1)*20*20 + (cifY-1)*20 + cifX ,1 ] <- (pages-1)*2 + box - 1\n          \n          for(px in 1:(xStepT-2))\n          {\n            for(py in 1:(yStepT-2))\n            {\n              #here things are read in\n              dataMatrix[((pages-1)*2 + box - 1)*20*20 + (cifY-1)*20 + cifX ,1 + (px-1)*(yStepT-2) + py ] <- prepared[[pages]][aYbase+py+1,aXbase+px+1]\n              \n            }\n          }\n        }\n      }\n    }\n  }\n  \n  return(dataMatrix)\n}\n",
    "created" : 1486893744739.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1023113451",
    "id" : "4D69F20D",
    "lastKnownWriteTime" : 1486624439,
    "last_content_update" : 1486624439,
    "path" : "~/dev/machine-learning/excersise1/trunk/Basefolder/loadImage.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}